import numpy as np 
import pandas as pd
import os, glob, toml
from nilearn import image
from nilearn.regions import img_to_signals_labels
import networkx as nx
import numpy.ma as ma
from plotnine import *
import random 
from collections import Counter, defaultdict 
from sklearn.svm import SVC, SVR
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from subprocess import call 
from scipy.stats import sem

def unpack_conf(FIRdesignMat_conf_dir:str, postfMRIprep_conf_dir:str): 

    """
    This function takes in the configuration toml file and unpack it into structdict data structure 
    The configuration files contain important information that can be reused in subsequent analyses. 
    
    Parameters
    ----------
    FIRdesignMat_conf_dir: 
        The directory for FIRdesignMat_conf.toml generated by the `fir_design_matrix` module. 
        (see https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/fir_design_matrix.py)
    postfMRIprep_conf_dir: 
        The directory for postfMRIprep_pipeline_config.toml generated by the `preprocessing_pipeline` module. 
        (https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/preprocessing_pipeline.py)
    
    Returns
    --------
    fir_cfg: dictionary-like data structure, keys are parameters; values are experiment-specific information in the config file 
    preprocess_cfg: dictionary-like data structure, keys are parameters; values are experiment-specific information in the config file 
    """
    
    # load FIRdesignMat configuration file
    with open(FIRdesignMat_conf_dir, "r") as toml_file:
        fir_cfg = toml.load(toml_file)
    fir_cfg = recurseCreateStructDict(fir_cfg)
    fir_cfg = fir_cfg.PARAMETERS
    
    # load postfMRIprep configuration file
    with open(postfMRIprep_conf_dir, "r") as toml_file:
        preprocess_cfg = toml.load(toml_file)
    preprocess_cfg = recurseCreateStructDict(preprocess_cfg)
    preprocess_cfg = preprocess_cfg.PARAMETERS

    return fir_cfg, preprocess_cfg 

def load_sub_data(input_data, atlas, mask) -> np.ndarray:
    
    """
    This function uses the selected parcellation scheme to divide the input data into parcels 
    
    Parameters
    ----------
    input_data: 
        A string pointing to the 4D timeseries niimg-like object (most likely to be the residual activity data). 
    atlas: 
        A string pointing to the 3D predefine parcellation scheme. 
    mask: 
        A string pointing to the 3D subject functional mask. 

    Returns
    --------
    signal: a 2d numpy array, first dimension is Parcel and the second is TR.
    """
    
    signal, _ = img_to_signals_labels(input_data, atlas, mask)
    
    # make sure to transpose the signal, so that the first dimension is parcel (instead of TR). 
    return signal.T 

def detect_bad_frame(sub_dir, signal, order, run_prop, spike): 
    
    """
    Residual timeseries can be contaminated by head motion, this function systematically scans the residual timeseries, 
    identifying and masking the time points near a head motion spike based on framewise displacements measured by fMRIprep. 
    Subsequently, motion-corrected BGFC matrices can be computed using numpy.ma.corrcoef followed by Fisher z-transformation, 
    and this operation is available for both epoch- and condition-level analyses.
    
    Parameters 
    ----------
    sub_dir: 
        This is used to locate the directory that contains the fmriprep output confounds for this subject,
        Which should be `base_dir/derivative/sub_dir/func`
    signal: 
        The output of load_sub_data, the shape is (nparcel, ts) 
    order: 
        This information is contained in the configuration file 
        This is a dictionary, with keys being the name of the run and values being its order. 
        This is necessary to locate all the confound files in the derivative folder and to sort them in the same order as they will be concatenated and modeled.
        The name of a fMRIprep output confound file is sub-%s_task-%s_run-%s_desc-preproc_bold.nii, the keys here need to be specified as'task-%s_run-%s'. 
        For example, 'task-divPerFacePerTone_run-2'. 
    run_prop: 
        This information is contained in the configuration file 
        The proportion of frames with FD > 0.5 within each run, exceeding which removes the run (5%).
    spike: 
        This information is contained in the configuration file 
        The spike cutoff, meaning that if a frame has FD greater than cutoff (2mm), then it would be treated as a spike. 
        
    Return 
    -------
    ts: signal without bad frames, the shape is still (nparcel, ts), but bad frames are np.nan across all TR
    """

    # get confound files for the participant, order it as how 12 runs are being concatenated. 
    confound_files = glob.glob(os.path.join(sub_dir, "*_run-*_desc-confounds_timeseries.tsv"))
    confound_files.sort(key=lambda x: order['_'.join(x.split("_")[1:3])])
    
    # len(ts)=3984, if 0, then remove the corresponding column in the FIR matrix. 
    ts = []
    for f in confound_files: 
        df = pd.read_csv(f,sep = '\t')
        motion = list(df['framewise_displacement']) # get FD 
        outlier_prop = round(np.sum(np.array(motion)>0.5)/len(motion)*100,2) # %of frames that have FD>0.5
        
        if outlier_prop > run_prop: # remove the whole run (all frames are bad frames) 
            ts_run = [0 for _ in range(len(motion))]
        else:   
        # otherwise check spike, but at the beginning, all timepoints are good frames. 
            ts_run = [1 for _ in range(len(motion))]
            for idx, m in enumerate(motion): 
                if np.isnan(m) or m <= spike: pass # if the frist frame or not a spike 
                if m > spike: # if spike 
                    ts_run[idx-1], ts_run[idx] = 0, 0 # remove the previous and the current frame 
                    if idx + 1 < len(motion): # if not the last frame, remove the next frame 
                        ts_run[idx+1] = 0 
        ts += ts_run 
    
    # check length 
    if len(ts) != signal.shape[1]: 
        print("Timeseries length does not match")
        return 
    
    for idx, frame in enumerate(ts): # remove bad frames from timeseries 
        if frame == 1: 
            pass
        else: 
            signal[:, idx] = np.nan
    
    return signal

def parcellate_rmMotion_batch(FIRdesignMat_conf_dir:str, postfMRIprep_conf_dir:str, sub, atlas):

    """
    This function performs parcellation and remove motion frames for a list of subjects.
    Residual timeseries computed by postfMRIprep_pipeline was first parcellated using input atlas. 
    Then the output signals further cleaned to remove the influnces of head motion. 
    
    Parameters 
    ----------
    FIRdesignMat_conf_dir: 
        The directory for FIRdesignMat_conf.toml generated by the `fir_design_matrix` module. 
        (see https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/fir_design_matrix.py)
    postfMRIprep_conf_dir: 
        The directory for postfMRIprep_pipeline_config.toml generated by the `preprocessing_pipeline` module. 
        (https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/preprocessing_pipeline.py)
    sub: 
        A list of subject, no prefix (e.g., 1,2,14)
    atlas: 
        A string pointing to the 3D predefine parcellation scheme. 

    Return 
    -------
    ts: signal without bad frames, the shape is still (nparcel, ts), but bad frames are np.nan across all TR
    """
    # unpack configuration file for useful information 
    fir_cfg, preprocess_cfg = unpack_conf(FIRdesignMat_conf_dir, postfMRIprep_conf_dir)
    order, run_prop, spike = eval(fir_cfg.order), fir_cfg.spike_cutoff, fir_cfg.prop_spike_cutoff
    res_base_dir, derivative_base_dir = preprocess_cfg.output_dir, preprocess_cfg.base_dir

    # batch process
    activity_data = []
    for sid in sub: 
        # parcellation 
        data_dir = os.path.join(res_base_dir, f"sub-{sid:03d}", "FIR_residual", f"sub-{sid:03d}_res.nii.gz")
        mask = os.path.join(res_base_dir, f"sub-{sid:03d}", "task_shared_mask", f"sub-{sid:03d}_task-shared_brain-mask.nii.gz")
        sub_signal = load_sub_data(data_dir, atlas, mask) # a list of 2d array (nparcel, ts) 
        # clean motion 
        sub_dir = os.path.join(derivative_base_dir, "derivative", f'sub-{sid:03d}/func')
        sub_signal_nospike = detect_bad_frame(sub_dir, sub_signal, order, run_prop, spike)
        # add to this list 
        activity_data.append(sub_signal_nospike)
    activity_dat_np = np.stack(activity_data, axis = 0)
    return activity_dat_np

def define_epoch(FIRdesignMat_conf_dir:str, postfMRIprep_conf_dir:str): 

    """
    This function define the TRs for each epoch of each condition. This is necessary for separting the residual timeseries 
    into epochs of each condition. 
    
    Parameters 
    ----------
    FIRdesignMat_conf_dir: 
        The directory for FIRdesignMat_conf.toml generated by the `fir_design_matrix` module. 
        (see https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/fir_design_matrix.py)
    postfMRIprep_conf_dir: 
        The directory for postfMRIprep_pipeline_config.toml generated by the `preprocessing_pipeline` module. 
        (https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/preprocessing_pipeline.py)

    Return 
    -------
    epoch: 
        3d array with the shape of (# of condition, # of epoch, # of total TR). This array defines the specific TRs associated 
        with each epoch of each condition. For example, if each epoch contains 40 TR, then sum(epoch[0,0,:])=40
    """
    
    # unpack configuration file for useful information 
    fir_cfg, preprocess_cfg = unpack_conf(FIRdesignMat_conf_dir, postfMRIprep_conf_dir)
    epoch_cond = np.repeat(fir_cfg.conditions,fir_cfg.epoch_per_run*fir_cfg.rep)

    # create the epoch design matrix for a given run 
    per_epoch = np.repeat(1,fir_cfg.epoch_tr)
    epoch_shape_per_run = np.eye(fir_cfg.epoch_per_run, fir_cfg.epoch_per_run)
    per_run_task = np.kron(epoch_shape_per_run,per_epoch)
    dummy_in = np.zeros((fir_cfg.epoch_per_run,fir_cfg.run_leadingin_tr))
    dummy_out = np.zeros((fir_cfg.epoch_per_run,fir_cfg.run_leadingout_tr))
    per_run_full = np.concatenate((dummy_in, per_run_task,dummy_out), axis = 1)
    
    # epoch template, need to crop the blocks for each condition 
    run_shape_per_sub = np.eye(len(fir_cfg.conditions)*fir_cfg.rep,len(fir_cfg.conditions)*fir_cfg.rep)
    sub_epoch_template = np.kron(run_shape_per_sub,per_run_full)
    sub_epoch_template_df = pd.DataFrame(sub_epoch_template.T, columns = epoch_cond)

    # define TRs in each epoch 
    cond_slice = []
    for cond in fir_cfg.conditions: 
        df_copy = sub_epoch_template_df.copy(deep = True)
        col_index = np.nonzero((df_copy.columns == cond) == False)[0]
        df_copy.iloc[:,col_index] = df_copy.iloc[:,col_index].replace(1,0)
        df_copy = df_copy.T
        cond_slice.append(np.array(df_copy))
    sub_epoch = np.stack(cond_slice, axis = 0).astype(int)

    return sub_epoch 

def separate_epochs(activity_data, epoch_list):
    """ 
    This function is the first step of dividing the timeseries into list of epochs with condition labels, 
    using the experiment specific epoch list. Specifically, this function divide the timeseries by condition, 
    and extract all TRs within each condition. 
    
    Parameters
    ----------
    activity_data: 
        3D array in shape [nSub, nVoxels/nParcel, nTRs]
        The masked activity data organized in voxel*TR formats of all subjects.
    epoch_list: 
        List of 3D array in shape [condition, nEpochs, nTRs]
        Specification of epochs and conditions, assuming all subjects have the same number of epoch. len(epoch_list) equals the number of subjects.

    Returns
    -------
    raw_data: 
        List of 2D array in shape [nParcels, timepoints]
        The data organized in epochs. len(raw_data) equals (# of subjects) * (# of conditions per subject)
    labels:
        List of 1D array, which is the condition labels of the epochs
        len(labels) labels equals len(raw_data). Showing the 
    """
    raw_data = []
    labels = []
    nsub = activity_data.shape[0]
    activity_data = [activity_data[i,:, :] for i in range(nsub)] # turn stacked np array into a list 
    
    for sid in range(len(epoch_list)): #for a given subject
        epoch = epoch_list[sid] # get their npy array
        for cond in range(epoch.shape[0]): # for a given condition
            # for each condition
            sub_epoch = epoch[cond, :, :]
            ts = np.zeros(epoch.shape[2])
            for eid in range(epoch.shape[1]):
                r = np.sum(sub_epoch[eid, :])
                if r > 0:   # there is an epoch in this condition
                    # collapse all epoch of a condition to a single time series
                    ts[sub_epoch[eid, :] == 1] = 1
            ts = ts.astype(bool)
            mat_cond = activity_data[sid][:,ts]
            mat_cond = np.ascontiguousarray(mat_cond)
            raw_data.append(mat_cond)
            labels.append(cond)

    return raw_data, labels

def _divide_into_epoch(ts, epoch_length): 
    
    """
    This function divide full timeseries into same length epochs
    
    Parameters
    ----------
    ts: 
        a 2d array, first dimension is number of parcels and th second dimension is TR/epoch * epoch num 
        epoch_length: TR/epoch 
    
    yiled:
    ------
        a generater for TRs within each epoch. 
    
    """
    ts_len = ts.shape[1]
    for i in range(0, ts_len, epoch_length):  
        yield ts[:,i:i + epoch_length] 
        
def separate_epochs_per_condition(raw_data, labels, condition_label, epoch_list): 
    
    """
    This function is the second step of dividing the full (residual) timeseries into epochs. Sepcifically, it filtered 
    out all TRs of the same condition, then divided the TRs into epoch structure, based on the epoch_list. 
    
    Parameter
    ----------
    raw_data: 
        Output from function `separate_epochs`
    labels: 
        Second otput from function `separate_epochs`
    condition_label: 
        One of the condition in the `label` output from `separate_epochs`. If there were 6 conditions, 
        condition_labels are 0,1,2,3,4,5.
    epoch_list: 
        List of 3D array in shape [condition, nEpochs, nTRs]
        Specification of epochs and conditions, assuming all subjects have the same number of epoch. len(epoch_list) equals the number of subjects.
    
    Return
    ------
    cond_epoch_ts: 
        a list of 3d array. 
        The length of the list is the number of subject. Each array is of the shape (# of epochs of the condition, # of parcels, # of TR per epoch)
    
    """
    nsub = len(epoch_list) # the number of subjects 
    epoch_length = sum(epoch_list[0][0,0,:]) # count the number of TRs within each epoch 
    cond_raw_data = [raw_data[cond] for cond in np.where(np.array(labels) == condition_label)[0].tolist()] # extract all TRs within the same condition 
    cond_epoch_ts = []
    # then divide into epoch structure. 
    for sub in range(nsub):
        cond_epoch_ts.append(np.stack(list(_divide_into_epoch(cond_raw_data[sub], epoch_length)), axis = 0))
    return cond_epoch_ts

def compute_sub_cond_connectome_ztrans_nobadframe(epoch_data:np.ndarray):
    
    """
    This function computes the z-transformed connectome for each subject for each condition. 
    So each subject will have just 1 connectome each condition (averaged across epochs). 
    This epoch-level processing is useful building graphs and for measuring individual differences. 
    This function also cleans up unqualified epochs or unqualified frames: 
    1) If all frames within this epoch is np.nan, drop the epoch
    2) If some frames within this epoch is np.nan, drop the frame then compute the correlation matrix 
    
    Parameters: 
    -----------
    epoch_data: 
        A list of 2d array. 
        The length equals the number of participant. Each array is of the shape (# of epoch, # of parcels, # of TR per epoch)

    Yields: 
    --------
    sub_cond_connectome_ztrans:
        A generator of 2d arrays. nParcel by nParcel Z-transformed correlation for each subject (averaged across all epochs). generator length is nsub
    """
    
    # basic information
    sub_num = len(epoch_data) # the number of partcipants
    epoch_num = epoch_data[0].shape[0] # the number of epochs 
    
    for sub in range(sub_num): 

        # grab the subject's data, shape is [nEpoch,nParcel,nTR]
        sub_epoch = epoch_data[sub]
        sub_epoch_connectome = []
        for cur_epoch in range(epoch_num): 
            if np.isnan(sub_epoch[cur_epoch,:,:]).all(): # if this epoch is full of na, meaning this epoch (run) has been dropped.
                pass 
            else: 
                # compute connectome for each nonempty epoch 
                sub_epoch_connectome.append(ma.corrcoef(ma.masked_invalid(sub_epoch[cur_epoch,:,:])).filled())

        # averave CorMat across all epochs 
        sub_cond_connectome = np.mean(sub_epoch_connectome, axis = 0)
        np.fill_diagonal(sub_cond_connectome, 0) # fill the diagnal with 0 for graph construction later

        # Z transformation 
        sub_cond_connectome_ztrans = np.arctanh(sub_cond_connectome)

        yield sub_cond_connectome_ztrans
            
def compute_epoch_cond_connectome_ztrans_nobadframe(epoch_data:np.ndarray):
    
    """
    This function computes the z-transformed connectome for each epoch for each condition. 
    So each subject will have multiple connectomes for each condition. This epoch-level processing 
    is useful for training machine learning models. This function also cleans up unqualified epochs
    or unqualified frames: 
    1) If all frames within this epoch is np.nan, drop the epoch
    2) If some frames within this epoch is np.nan, drop the frame then compute the correlation matrix 
    
    Parameters: 
    -----------
    epoch_data: 
        A list of 2d array. 
        The length equals the number of participant. Each array is of the shape (# of epoch, # of parcels, # of TR per epoch)
                
    Yields: 
    --------
    all_connectomes: 
        A nested list. Each sub list is epoch level (nParcel by nParcel) correlation matrix for a participant. 
    """
    
    # basic information
    sub_num = len(epoch_data) # the number of partcipants
    epoch_num = epoch_data[0].shape[0] # the number of epochs 

    all_connectomes = []
    for sub in range(sub_num): 
        sub_connectomes = []
        # grab the subject's data, shape is [16,200,36]
        sub_epoch = epoch_data[sub]
        for cur_epoch in range(epoch_num): 
            if np.isnan(sub_epoch[cur_epoch,:,:]).all(): # if this epoch is full of na, meaning this epoch (run) has been dropped.
                pass 
            else: 
                # compute connectome for each nonempty epoch 
                epoch_cond_connectome = ma.corrcoef(ma.masked_invalid(sub_epoch[cur_epoch,:,:])).filled()
                np.fill_diagonal(epoch_cond_connectome, 0) # fill the diagnal with 0 for graph construction later
                epoch_cond_connectome_ztrans = np.arctanh(epoch_cond_connectome) # Z transformation 
                sub_connectomes.append(epoch_cond_connectome_ztrans)
        all_connectomes.append(sub_connectomes)
    return all_connectomes 

def vectorize_connectome(connectome_list):
    
    """
    This function extracts the upper triangle of any connectome, which will potentially serve as features 
    for training machine learning models. 
    
    Parameters: 
    -----------
    connectome_list: 
        A nested list of connectomes (2d array of the shape nParcel x nParcel)
                
    Returns: 
    --------
    connectome_vectors:
        A list (len = subject) of list (len = epoch) of vectorized connectom(upper triangle, including diagnol)
    """
    sub_epoch_vector = []
    for sub_conn_list in connectome_list: 
        
        nparcel = sub_conn_list[0].shape[0]
        vectorized_conn = [list(c[np.triu_indices(nparcel, k = 1)]) for c in sub_conn_list] 

        sub_epoch_vector.append(vectorized_conn)
    
    return sub_epoch_vector 

def plot_parcel_FIR_estimates(FIRdesignMat_conf_dir, postfMRIprep_conf_dir, parcel_id, sub_list, atlas): 

    """
    This function plots the estimates of FIR regressors across all subjects. These plots should serve 
    as sanity checks for FIR model efficiency. The plot should reveal boxcart shape reflecting condition structure.
    
    Parameters 
    ----------
    FIRdesignMat_conf_dir: 
        The directory for FIRdesignMat_conf.toml generated by the `fir_design_matrix` module. 
        (see https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/fir_design_matrix.py)
    postfMRIprep_conf_dir: 
        The directory for postfMRIprep_pipeline_config.toml generated by the `preprocessing_pipeline` module. 
        (https://github.com/peetal/bgfc_kit/blob/main/bgfc_kit/preprocessing_pipeline.py)
    sub_list: 
        A list of subject, no prefix (e.g., 1,2,14)
    atlas: 
        A string pointing to the 3D predefine parcellation scheme. 

    Return 
    ----------
    plot: 
        FIR estimates for a specific parcel. 
    """

    # unpack configuration files for important information
    fir_cfg, preprocess_cfg = unpack_conf(FIRdesignMat_conf_dir, postfMRIprep_conf_dir)
    base_dir = preprocess_cfg.output_dir
    
    signal_array = []
    for sub in sub_list:
 
        # file names for each participant 
        beta_img = os.path.join(base_dir, f"sub-{sub:03d}", "FIR_betas", f"sub-{sub:03d}_beta.nii.gz")
        mask = os.path.join(base_dir, f"sub-{sub:03d}", "task_shared_mask", f"sub-{sub:03d}_task-shared_brain-mask.nii.gz")
    
        # apply parcellation
        signals, labels = img_to_signals_labels(beta_img, atlas, mask)
        signal_array.append(signals)
    
    # sub, regressor, region 
    signal_array = np.stack(signal_array, axis = 0)
    
    #compute mean and se for the sample
    signal_se = sem(signal_array, 0)
    signal_mean = np.mean(signal_array, 0)
    lower_bound = signal_mean - signal_se
    upper_bound = signal_mean + signal_se
    
    # plot 
    fig, ax = plt.subplots(figsize=(20,3))
    plt.plot(signal_mean[:,parcel_id])
    plt.fill_between(range(0, 216), lower_bound[:,parcel_id], upper_bound[:,parcel_id], alpha=.3)
    plt.show()
        
        
def construct_graphs(corMats, threshold=0):
    
    """
    This function construct a unthresholded, weighted, graph for each connectome
    
    Parameters: 
    -----------
    corMat_list: 
        A list of connectome (i.e., correlation matrix)
    threshold: 
        Whether to include the edge, default is 0

    Returns: 
    --------
    graph_list: 
        A list of networkX graphs created based on the input correlation matrices. 
    """
    
    def _do_single_graph(corMat, threshold):
        
        # Preset the graph
        G = nx.Graph()

        # Create the edge list
        nodelist = []
        edgelist = []
        
        for row_counter in range(corMat.shape[0]):
            nodelist.append(str(row_counter))  # Set up the node names (zero indexed)
            for col_counter in range(corMat.shape[1]):

                # Determine whether to include the edge based on whether it exceeds the threshold
                if abs(corMat[row_counter, col_counter]) > abs(threshold):
                    # Add a tuple specifying the voxel pairs being compared and the weight of the edge
                    edgelist.append((str(row_counter), str(col_counter), {'coupling_strength': corMat[row_counter, col_counter]}))
                    #edgelist.append((str(row_counter), str(col_counter), {'weight': 1}))

        # Create the nodes in the graph
        G.add_nodes_from(nodelist)

        # Add the edges
        G.add_edges_from(edgelist)
        
        nx.set_edge_attributes(G, {e: 1/d["coupling_strength"] for e, d in G.edges.items()}, "distance")
    
        return(G)
        
    if type(corMats) == list: # if input is a list of corMat, then return a list of graph 
        graph_list = [_do_single_graph(corMat, threshold) for corMat in corMats]
    else: # otherwise, just return a single graph. 
        graph_list = _do_single_graph(corMats, threshold)

    return graph_list

def compute_threshold(corMat, density):
    """
    This function computes threshold given edge density (i.e., the percentage of edges to keep). 
    
    Parameters: 
    -----------
    corMat: 
        One correlation matrix 
    density: 
        The percentage of edges to keep (e.g., 15 means to keep the top 15%)
    
    Returns: 
    --------
    threshold: 
        The threshold computed based on the corMat and density

    """
    
    def _upper_tri_indexing(A):
        m = A.shape[0]
        r,c = np.triu_indices(m,1)
        return A[r,c]
    threshold = np.percentile(_upper_tri_indexing(corMat),100-density)
    return threshold
    

def construct_threshold_binary_graphs(corMats, density):
    
    """
    This function construct thresholded, binary, graph for each connectome
    
    Parameters: 
    -----------
    corMat_list: 
        A list of connectome (i.e., correlation matrix)
    density: 
        The percentage of edges to keep (e.g., 15 means to keep the top 15%)

    Returns: 
    --------
    graph_list: 
        A list of networkX graphs created based on the input correlation matrices. 
    """
    def _do_single_graph(corMat, density):
        
        threshold = compute_threshold(corMat, density)
        
        # Preset the graph
        G = nx.Graph()

        # Create the edge list
        nodelist = []
        edgelist = []
        
        for row_counter in range(corMat.shape[0]):
            nodelist.append(str(row_counter))  # Set up the node names (zero indexed)
            for col_counter in range(corMat.shape[1]):

                # Determine whether to include the edge based on whether it exceeds the threshold -> constructing binary graph
                if corMat[row_counter, col_counter] >= threshold:
                    edgelist.append((str(row_counter), str(col_counter), {'weight': 1}))

        # Create the nodes in the graph
        G.add_nodes_from(nodelist)

        # Add the edges
        G.add_edges_from(edgelist)
        
        return(G)
        
    if type(corMats) == list: # if input is a list of corMat, then return a list of graph 
        graph_list = [_do_single_graph(corMat, density) for corMat in corMats]
    else: # otherwise, just return a single graph. 
        graph_list = _do_single_graph(corMats, density)

    return graph_list


def participation_coefficient(G, module_partition):
    '''
    Computes the participation coefficient of nodes of G with partition
    defined by module_partition.
    (Guimera et al. 2005).

    Parameters
    ----------
    G:
        Class of networkx.Graph
    module_partition:
        A dictionary mapping each community name to a list of nodes in G

    Returns
    -------
    dict:
        A dictionary mapping the nodes of G to their participation coefficient 
        under the participation specified by module_partition.
    '''
    # Initialise dictionary for the participation coefficients
    pc_dict = {}

    # Loop over modules to calculate participation coefficient for each node
    for m in module_partition.keys():
        # Create module subgraph
        M = set(module_partition[m])
        for v in M:
            # Calculate the degree of v in G
            degree = float(nx.degree(G=G, nbunch=v))

            # Calculate the number of intramodule degree of v
            wm_degree = float(sum([1 for u in M if (u, v) in G.edges()]))
            
            if degree == 0 and wm_degree == 0: 
                pc_dict[v] = 0
            else: 
            # The participation coeficient is 1 - the square of
            # the ratio of the within module degree and the total degree
                pc_dict[v] = 1 - ((float(wm_degree) / float(degree))**2)

    return pc_dict
